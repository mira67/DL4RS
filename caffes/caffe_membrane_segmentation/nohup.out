[train]:    Evaluating on validation data (3317760 pixels)...
Traceback (most recent call last):
  File "./coca/train.py", line 491, in <module>
    omitLabels=[-1], Xvalid=Xvalid, Yvalid=Yvalid, syn_func=syn_func)
  File "./coca/train.py", line 292, in _training_loop
    rv = solver.net.forward()
  File "/home/mirabot/Downloads/caffe-master/python/caffe/pycaffe.py", line 121, in _Net_forward
    self._forward(start_ind, end_ind)
KeyboardInterrupt
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1108 12:50:00.074870 10563 solver.cpp:48] Initializing solver from parameters: 
base_lr: 0.001
display: 200
max_iter: 1
lr_policy: "step"
gamma: 0.1
momentum: 0
stepsize: 70000
snapshot: 2000
snapshot_prefix: "./"
solver_mode: CPU
net: "n3-net.prototxt"
I1108 12:50:00.075014 10563 solver.cpp:91] Creating training net from net file: n3-net.prototxt
I1108 12:50:00.075193 10563 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: n3-net.prototxt
I1108 12:50:00.075242 10563 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1108 12:50:00.075390 10563 net.cpp:58] Initializing net from parameters: 
name: "CiresanN3"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "MemoryData"
  top: "data"
  top: "label"
  memory_data_param {
    batch_size: 100
    channels: 1
    height: 65
    width: 65
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 200
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "prob"
  type: "Softmax"
  bottom: "ip2"
  top: "prob"
}
I1108 12:50:00.075474 10563 layer_factory.hpp:77] Creating layer data
I1108 12:50:00.075486 10563 net.cpp:100] Creating Layer data
I1108 12:50:00.075491 10563 net.cpp:408] data -> data
I1108 12:50:00.075518 10563 net.cpp:408] data -> label
I1108 12:50:00.075819 10563 net.cpp:150] Setting up data
I1108 12:50:00.075829 10563 net.cpp:157] Top shape: 100 1 65 65 (422500)
I1108 12:50:00.075834 10563 net.cpp:157] Top shape: 100 (100)
I1108 12:50:00.075836 10563 net.cpp:165] Memory required for data: 1690400
I1108 12:50:00.075840 10563 layer_factory.hpp:77] Creating layer label_data_1_split
I1108 12:50:00.075847 10563 net.cpp:100] Creating Layer label_data_1_split
I1108 12:50:00.075851 10563 net.cpp:434] label_data_1_split <- label
I1108 12:50:00.075857 10563 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1108 12:50:00.075865 10563 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1108 12:50:00.075882 10563 net.cpp:150] Setting up label_data_1_split
I1108 12:50:00.075887 10563 net.cpp:157] Top shape: 100 (100)
I1108 12:50:00.075891 10563 net.cpp:157] Top shape: 100 (100)
I1108 12:50:00.075893 10563 net.cpp:165] Memory required for data: 1691200
I1108 12:50:00.075896 10563 layer_factory.hpp:77] Creating layer conv1
I1108 12:50:00.075906 10563 net.cpp:100] Creating Layer conv1
I1108 12:50:00.075909 10563 net.cpp:434] conv1 <- data
I1108 12:50:00.075916 10563 net.cpp:408] conv1 -> conv1
I1108 12:50:00.076364 10563 net.cpp:150] Setting up conv1
I1108 12:50:00.076380 10563 net.cpp:157] Top shape: 100 48 61 61 (17860800)
I1108 12:50:00.076385 10563 net.cpp:165] Memory required for data: 73134400
I1108 12:50:00.076395 10563 layer_factory.hpp:77] Creating layer pool1
I1108 12:50:00.076401 10563 net.cpp:100] Creating Layer pool1
I1108 12:50:00.076407 10563 net.cpp:434] pool1 <- conv1
I1108 12:50:00.076426 10563 net.cpp:408] pool1 -> pool1
I1108 12:50:00.076434 10563 net.cpp:150] Setting up pool1
I1108 12:50:00.076438 10563 net.cpp:157] Top shape: 100 48 60 60 (17280000)
I1108 12:50:00.076442 10563 net.cpp:165] Memory required for data: 142254400
I1108 12:50:00.076445 10563 layer_factory.hpp:77] Creating layer relu1
I1108 12:50:00.076452 10563 net.cpp:100] Creating Layer relu1
I1108 12:50:00.076454 10563 net.cpp:434] relu1 <- pool1
I1108 12:50:00.076459 10563 net.cpp:395] relu1 -> pool1 (in-place)
I1108 12:50:00.076464 10563 net.cpp:150] Setting up relu1
I1108 12:50:00.076468 10563 net.cpp:157] Top shape: 100 48 60 60 (17280000)
I1108 12:50:00.076472 10563 net.cpp:165] Memory required for data: 211374400
I1108 12:50:00.076474 10563 layer_factory.hpp:77] Creating layer norm1
I1108 12:50:00.076479 10563 net.cpp:100] Creating Layer norm1
I1108 12:50:00.076483 10563 net.cpp:434] norm1 <- pool1
I1108 12:50:00.076488 10563 net.cpp:408] norm1 -> norm1
I1108 12:50:00.076504 10563 net.cpp:150] Setting up norm1
I1108 12:50:00.076509 10563 net.cpp:157] Top shape: 100 48 60 60 (17280000)
I1108 12:50:00.076513 10563 net.cpp:165] Memory required for data: 280494400
I1108 12:50:00.076515 10563 layer_factory.hpp:77] Creating layer conv2
I1108 12:50:00.076522 10563 net.cpp:100] Creating Layer conv2
I1108 12:50:00.076526 10563 net.cpp:434] conv2 <- norm1
I1108 12:50:00.076531 10563 net.cpp:408] conv2 -> conv2
I1108 12:50:00.077155 10563 net.cpp:150] Setting up conv2
I1108 12:50:00.077165 10563 net.cpp:157] Top shape: 100 48 56 56 (15052800)
I1108 12:50:00.077168 10563 net.cpp:165] Memory required for data: 340705600
I1108 12:50:00.077177 10563 layer_factory.hpp:77] Creating layer relu2
I1108 12:50:00.077180 10563 net.cpp:100] Creating Layer relu2
I1108 12:50:00.077184 10563 net.cpp:434] relu2 <- conv2
I1108 12:50:00.077195 10563 net.cpp:395] relu2 -> conv2 (in-place)
I1108 12:50:00.077201 10563 net.cpp:150] Setting up relu2
I1108 12:50:00.077205 10563 net.cpp:157] Top shape: 100 48 56 56 (15052800)
I1108 12:50:00.077208 10563 net.cpp:165] Memory required for data: 400916800
I1108 12:50:00.077211 10563 layer_factory.hpp:77] Creating layer pool2
I1108 12:50:00.077217 10563 net.cpp:100] Creating Layer pool2
I1108 12:50:00.077220 10563 net.cpp:434] pool2 <- conv2
I1108 12:50:00.077225 10563 net.cpp:408] pool2 -> pool2
I1108 12:50:00.077234 10563 net.cpp:150] Setting up pool2
I1108 12:50:00.077239 10563 net.cpp:157] Top shape: 100 48 55 55 (14520000)
I1108 12:50:00.077241 10563 net.cpp:165] Memory required for data: 458996800
I1108 12:50:00.077244 10563 layer_factory.hpp:77] Creating layer norm2
I1108 12:50:00.077250 10563 net.cpp:100] Creating Layer norm2
I1108 12:50:00.077252 10563 net.cpp:434] norm2 <- pool2
I1108 12:50:00.077257 10563 net.cpp:408] norm2 -> norm2
I1108 12:50:00.077270 10563 net.cpp:150] Setting up norm2
I1108 12:50:00.077275 10563 net.cpp:157] Top shape: 100 48 55 55 (14520000)
I1108 12:50:00.077278 10563 net.cpp:165] Memory required for data: 517076800
I1108 12:50:00.077281 10563 layer_factory.hpp:77] Creating layer conv3
I1108 12:50:00.077288 10563 net.cpp:100] Creating Layer conv3
I1108 12:50:00.077291 10563 net.cpp:434] conv3 <- norm2
I1108 12:50:00.077297 10563 net.cpp:408] conv3 -> conv3
I1108 12:50:00.077931 10563 net.cpp:150] Setting up conv3
I1108 12:50:00.077942 10563 net.cpp:157] Top shape: 100 48 51 51 (12484800)
I1108 12:50:00.077947 10563 net.cpp:165] Memory required for data: 567016000
I1108 12:50:00.077957 10563 layer_factory.hpp:77] Creating layer relu3
I1108 12:50:00.077970 10563 net.cpp:100] Creating Layer relu3
I1108 12:50:00.077975 10563 net.cpp:434] relu3 <- conv3
I1108 12:50:00.077980 10563 net.cpp:395] relu3 -> conv3 (in-place)
I1108 12:50:00.077988 10563 net.cpp:150] Setting up relu3
I1108 12:50:00.077994 10563 net.cpp:157] Top shape: 100 48 51 51 (12484800)
I1108 12:50:00.077998 10563 net.cpp:165] Memory required for data: 616955200
I1108 12:50:00.078003 10563 layer_factory.hpp:77] Creating layer pool3
I1108 12:50:00.078009 10563 net.cpp:100] Creating Layer pool3
I1108 12:50:00.078013 10563 net.cpp:434] pool3 <- conv3
I1108 12:50:00.078019 10563 net.cpp:408] pool3 -> pool3
I1108 12:50:00.078029 10563 net.cpp:150] Setting up pool3
I1108 12:50:00.078035 10563 net.cpp:157] Top shape: 100 48 50 50 (12000000)
I1108 12:50:00.078040 10563 net.cpp:165] Memory required for data: 664955200
I1108 12:50:00.078044 10563 layer_factory.hpp:77] Creating layer ip1
I1108 12:50:00.078052 10563 net.cpp:100] Creating Layer ip1
I1108 12:50:00.078058 10563 net.cpp:434] ip1 <- pool3
I1108 12:50:00.078064 10563 net.cpp:408] ip1 -> ip1
I1108 12:50:00.322788 10563 net.cpp:150] Setting up ip1
I1108 12:50:00.322821 10563 net.cpp:157] Top shape: 100 200 (20000)
I1108 12:50:00.322824 10563 net.cpp:165] Memory required for data: 665035200
I1108 12:50:00.322835 10563 layer_factory.hpp:77] Creating layer relu4
I1108 12:50:00.322845 10563 net.cpp:100] Creating Layer relu4
I1108 12:50:00.322849 10563 net.cpp:434] relu4 <- ip1
I1108 12:50:00.322854 10563 net.cpp:395] relu4 -> ip1 (in-place)
I1108 12:50:00.322862 10563 net.cpp:150] Setting up relu4
I1108 12:50:00.322866 10563 net.cpp:157] Top shape: 100 200 (20000)
I1108 12:50:00.322868 10563 net.cpp:165] Memory required for data: 665115200
I1108 12:50:00.322871 10563 layer_factory.hpp:77] Creating layer ip2
I1108 12:50:00.322877 10563 net.cpp:100] Creating Layer ip2
I1108 12:50:00.322880 10563 net.cpp:434] ip2 <- ip1
I1108 12:50:00.322885 10563 net.cpp:408] ip2 -> ip2
I1108 12:50:00.322901 10563 net.cpp:150] Setting up ip2
I1108 12:50:00.322904 10563 net.cpp:157] Top shape: 100 2 (200)
I1108 12:50:00.322907 10563 net.cpp:165] Memory required for data: 665116000
I1108 12:50:00.322916 10563 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1108 12:50:00.322923 10563 net.cpp:100] Creating Layer ip2_ip2_0_split
I1108 12:50:00.322926 10563 net.cpp:434] ip2_ip2_0_split <- ip2
I1108 12:50:00.322957 10563 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1108 12:50:00.322964 10563 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1108 12:50:00.322969 10563 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_2
I1108 12:50:00.322978 10563 net.cpp:150] Setting up ip2_ip2_0_split
I1108 12:50:00.322981 10563 net.cpp:157] Top shape: 100 2 (200)
I1108 12:50:00.322983 10563 net.cpp:157] Top shape: 100 2 (200)
I1108 12:50:00.322988 10563 net.cpp:157] Top shape: 100 2 (200)
I1108 12:50:00.322989 10563 net.cpp:165] Memory required for data: 665118400
I1108 12:50:00.322993 10563 layer_factory.hpp:77] Creating layer accuracy
I1108 12:50:00.322996 10563 net.cpp:100] Creating Layer accuracy
I1108 12:50:00.322999 10563 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I1108 12:50:00.323002 10563 net.cpp:434] accuracy <- label_data_1_split_0
I1108 12:50:00.323007 10563 net.cpp:408] accuracy -> accuracy
I1108 12:50:00.323012 10563 net.cpp:150] Setting up accuracy
I1108 12:50:00.323016 10563 net.cpp:157] Top shape: (1)
I1108 12:50:00.323019 10563 net.cpp:165] Memory required for data: 665118404
I1108 12:50:00.323021 10563 layer_factory.hpp:77] Creating layer loss
I1108 12:50:00.323025 10563 net.cpp:100] Creating Layer loss
I1108 12:50:00.323029 10563 net.cpp:434] loss <- ip2_ip2_0_split_1
I1108 12:50:00.323031 10563 net.cpp:434] loss <- label_data_1_split_1
I1108 12:50:00.323035 10563 net.cpp:408] loss -> loss
I1108 12:50:00.323041 10563 layer_factory.hpp:77] Creating layer loss
I1108 12:50:00.323051 10563 net.cpp:150] Setting up loss
I1108 12:50:00.323055 10563 net.cpp:157] Top shape: (1)
I1108 12:50:00.323056 10563 net.cpp:160]     with loss weight 1
I1108 12:50:00.323065 10563 net.cpp:165] Memory required for data: 665118408
I1108 12:50:00.323073 10563 layer_factory.hpp:77] Creating layer prob
I1108 12:50:00.323078 10563 net.cpp:100] Creating Layer prob
I1108 12:50:00.323081 10563 net.cpp:434] prob <- ip2_ip2_0_split_2
I1108 12:50:00.323086 10563 net.cpp:408] prob -> prob
I1108 12:50:00.323094 10563 net.cpp:150] Setting up prob
I1108 12:50:00.323097 10563 net.cpp:157] Top shape: 100 2 (200)
I1108 12:50:00.323101 10563 net.cpp:165] Memory required for data: 665119208
I1108 12:50:00.323102 10563 net.cpp:228] prob does not need backward computation.
I1108 12:50:00.323107 10563 net.cpp:226] loss needs backward computation.
I1108 12:50:00.323109 10563 net.cpp:228] accuracy does not need backward computation.
I1108 12:50:00.323113 10563 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1108 12:50:00.323115 10563 net.cpp:226] ip2 needs backward computation.
I1108 12:50:00.323118 10563 net.cpp:226] relu4 needs backward computation.
I1108 12:50:00.323122 10563 net.cpp:226] ip1 needs backward computation.
I1108 12:50:00.323124 10563 net.cpp:226] pool3 needs backward computation.
I1108 12:50:00.323127 10563 net.cpp:226] relu3 needs backward computation.
I1108 12:50:00.323129 10563 net.cpp:226] conv3 needs backward computation.
I1108 12:50:00.323133 10563 net.cpp:226] norm2 needs backward computation.
I1108 12:50:00.323137 10563 net.cpp:226] pool2 needs backward computation.
I1108 12:50:00.323138 10563 net.cpp:226] relu2 needs backward computation.
I1108 12:50:00.323142 10563 net.cpp:226] conv2 needs backward computation.
I1108 12:50:00.323144 10563 net.cpp:226] norm1 needs backward computation.
I1108 12:50:00.323148 10563 net.cpp:226] relu1 needs backward computation.
I1108 12:50:00.323150 10563 net.cpp:226] pool1 needs backward computation.
I1108 12:50:00.323153 10563 net.cpp:226] conv1 needs backward computation.
I1108 12:50:00.323156 10563 net.cpp:228] label_data_1_split does not need backward computation.
I1108 12:50:00.323160 10563 net.cpp:228] data does not need backward computation.
I1108 12:50:00.323163 10563 net.cpp:270] This network produces output accuracy
I1108 12:50:00.323165 10563 net.cpp:270] This network produces output loss
I1108 12:50:00.323168 10563 net.cpp:270] This network produces output prob
I1108 12:50:00.323184 10563 net.cpp:283] Network initialization done.
I1108 12:50:00.323261 10563 solver.cpp:60] Solver scaffolding done.
[train]: batch shape: [100   1  65  65]
[train]: loading file: membranes_isbi2012/train-volume.tif
[train]: loading file: membranes_isbi2012/train-labels.tif
[train]: yAll is [0.0, 255.0]
[train]: 0 pixels will be omitted

[train]: training data shape: (20, 576, 576)
[train]: validation data shape: (10, 576, 576)
[train]: mask shape: (20, 576, 576)
conv1[0] : (48, 1, 5, 5)
conv1[1] : (48,)
conv2[0] : (48, 48, 5, 5)
conv2[1] : (48,)
conv3[0] : (48, 48, 5, 5)
conv3[1] : (48,)
ip1[0] : (200, 120000)
ip1[1] : (200,)
ip2[0] : (2, 200)
ip2[1] : (2,)
Do Training
True
[emlib]: num. pixels per class label is: [1200085, 4042795]
[emlib]: will draw 1200085 samples from each class
[train]: completed iteration 1 (of 1; 0.45 min elapsed; 0.34 CNN min)
[train]:    epoch: 1 (0.00), loss: 0.693, acc: 0.510, learn rate: 1.000e-03
